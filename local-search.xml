<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>LLM_review</title>
    <link href="/2023/12/23/LLM-review/"/>
    <url>/2023/12/23/LLM-review/</url>
    
    <content type="html"><![CDATA[<h1>大模型常见知识点</h1><h2 id="深度学习基础">深度学习基础</h2><h3 id="混合精度-BF16">混合精度&amp;BF16</h3><p>混合精度是指训练时在模型中同时使用 16 位和 32 位浮点类型，从而加快运行速度，减少内存使用的一种训练方法。通过让模型的某些部分保持使用 32 位类型以保持数值稳定性，可以<strong>缩短模型的单步用时，而在评估指标（如准确率）方面仍可以获得同等的训练效果</strong>。</p><ul class="lvl-0"><li class="lvl-2"><p>进行精度选择</p></li><li class="lvl-3"><p>例如：f(x) &gt;&gt; x时，FP32 如：Exp, Log ,Cross-entropy 这些操作可能导致在FW和BW时结果有溢出</p></li><li class="lvl-4"><p>RelU, sigmoid对于FP16 计算安全</p></li><li class="lvl-4"><p>大型规约、Loss计算应在FP32中进行</p></li></ul><p>BF16和PF16都是16位浮点数格式</p><ul class="lvl-0"><li class="lvl-2"><p><strong>BF16</strong></p><ul class="lvl-2"><li class="lvl-4"><strong>1</strong> 符号位 8指数位 7尾数位</li><li class="lvl-4">较大的数值范围</li><li class="lvl-4">降低内存占用</li></ul></li><li class="lvl-2"><p>FP16</p><ul class="lvl-2"><li class="lvl-4">1 符号位 5指数位 10 尾数位</li><li class="lvl-4">精度高</li></ul></li></ul><h3 id="模型参数量计算">模型参数量计算</h3><p>针对参数量大小为<strong>x</strong>的模型，使用混合精度模型训练</p><ul class="lvl-0"><li class="lvl-2"><p>参数fp16: 2x</p></li><li class="lvl-2"><p>梯度fp16:2x</p></li><li class="lvl-2"><p>优化器状态：12x</p><ul class="lvl-2"><li class="lvl-4">模型参数fp32:4x</li><li class="lvl-4">优化器动量fp32：4x</li><li class="lvl-4">优化器方差：4x</li></ul></li></ul><h3 id="Normalization-归一化">Normalization 归一化</h3><ul class="lvl-0"><li class="lvl-2"><p>BN : 一个batch在同一纬度上做处理，所有样本的每一个特征之上进行归一化，规范化分布，解决内部<strong>协方差</strong>偏移，缓解梯度饱和问题，加快收敛。（沿着channel方向，标准化图）BN处理一个小批量（batch）中所有样本的同一特征。它计算小批量中每个特征的均值和方差，并使用这些统计信息来归一化特征。</p><ul class="lvl-2"><li class="lvl-4">在CV卷积核中使用，是为了使得在不同样本上提取特征的稳定性</li><li class="lvl-4">抹杀了不同特征之间的大小关系，保留了不同样本间的大小关系</li><li class="lvl-4">NLP不定长，且好多位置填0，影响其他非0参数计算</li></ul></li><li class="lvl-2"><p>LN：在每个特征维度上进行归一化（在每个句子间） LN在单个样本内跨所有特征进行归一化。与BN不同，它不依赖于小批量中的其他样本。</p><ul class="lvl-2"><li class="lvl-4">batch内不同样本同一位置token之间的差异性很重要，保证在不同token上提取的稳定性</li></ul></li><li class="lvl-2"><p>IN: IN常用于风格迁移等任务，它在单个样本的每个通道上进行归一化</p></li><li class="lvl-2"><p>GN:GN是BN和LN的折衷方案。它将特征分成若干组，并在这些组内进行归一化</p></li><li class="lvl-2"><p>适用场景不同：BN用于CNN，LN用于RNN，IN用于样式迁移，GN兼具BN和LN的优点</p></li><li class="lvl-2"><p>归一化粒度不同：BN针对批，LN针对层，IN针对实例，GN针对通道组</p></li><li class="lvl-2"><p>计算量不同：BN和GN计算量大，LN和IN计算量小</p></li></ul><h3 id="PreNorm-PostNorm">PreNorm &amp; PostNorm</h3><ul class="lvl-0"><li class="lvl-2"><p>PreNorm</p><ul class="lvl-2"><li class="lvl-6">在 prenorm 中，归一化操作在层的输入之前进行。具体而言，在输入进入层之前，首先应用层归一化操作。这样，归一化的计算会涉及到输入以及层的权重和偏置参数。</li><li class="lvl-4">这种方式的一个优势是能够更好地处理梯度传播，因为输入进入网络时已经被归一化，梯度信号更容易传递。但有时可能会受到初始归一化对梯度的影响。</li><li class="lvl-4">容易训练，但是深层网络学习不好，因为权重大多来自恒等分支，恒等分支容易训练</li></ul></li><li class="lvl-2"><p>PostNorm</p><ul class="lvl-2"><li class="lvl-6">先将上一层的输出和当前层的输出相加之后，在进行归一化</li><li class="lvl-4">这种方式的优势在于，归一化操作不会影响输入到层的计算，因此在训练初期可能更容易收敛。然而，在梯度传播方面可能存在一些挑战。</li><li class="lvl-4">训练效果好，但是不容易训练（相当于残差部分的功能被削弱了）</li></ul></li></ul><h3 id="协方差偏移">协方差偏移</h3><p><strong>协方差偏移（Covariate Shift</strong>是指在机器学习中训练数据和测试数据之间的分布不匹配的情况。当训练数据和测试数据的分布差异较大时，<strong>模型可能面临协方差偏移问题，这可能导致模型在测试集上的性能下降。</strong></p><p>具体而言，协方差偏移发生在训练数据和测试数据的输入特征分布不同，但目标分布（标签分布）保持相对稳定的情况。这可能是由于数据的收集方式、来源的变化或者时间上的漂移等原因引起的。归一化能够将输入的分布特征调节的相对一致。</p><h3 id="正则化和范数">正则化和范数</h3><p>正则化是通过在模型的损失函数中添加一个附加项（正则化项）来控制模型的复杂性的一种技术。通过约束参数大小，降低过拟合，降低模型复杂度。</p><ul class="lvl-0"><li class="lvl-2"><p>L1 <strong>在模型的损失函数中添加模型权重的绝对值之和</strong></p><ul class="lvl-2"><li class="lvl-6"><strong>有助于一些权重为0，从而实现特征选择，减少模型的复杂性</strong></li><li class="lvl-4">适用于特征稀疏性强的情况</li><li class="lvl-4">L1 范数倾向于使一些参数变为零，因此适用于特征选择和稀疏性推导。</li></ul></li><li class="lvl-2"><p>L2 <strong>在模型的损失函数中添加模型权重的平方和</strong></p><ul class="lvl-2"><li class="lvl-6"><strong>有助于防止权重过大，使模型更加稳定，使得所有特征对于预测都有一定贡献</strong></li><li class="lvl-4">适用于所有特征对预测有一定贡献的情况</li><li class="lvl-4">L2 范数倾向于使参数都变得较小，适用于权重衰减和防止过拟合。</li></ul></li></ul><h3 id="FW-BW">FW &amp; BW</h3><ul class="lvl-0"><li class="lvl-2"><p>前向传播</p></li></ul><p>前向传播指的是从输入层到输出层依次计算每一层的输出结果的过程。在前向传播中，输入数据通过神经网络的各个层，逐层进行加权求和、激活函数处理，最终得到输出结果。<strong>说的简单一点就是，前向传播就是把输入数据转换为输出结果，实现对数据的分类和预测。</strong></p><ul class="lvl-0"><li class="lvl-2"><p>反向传播</p></li></ul><p>反向传播算法将损失函数对网络输出的偏导数作为初始误差信号，并通过链式法则，将这个误差信号反向传播到每个神经元的输入端，计算每个神经元的输入对其输出的偏导数，并将这个偏导数与误差信号相乘，得到每个神经元的误差信号。最终，反向传播算法计算每个权重和偏差的梯度，用于更新网络的参数。简单来说，反向传播就是过计算损失函数对网络参数的梯度，优化神经网络的参数，提高网络的性能。</p><h3 id="激活函数">激活函数</h3><p><strong>激活函数在神经网络的每一层中引入非线性，使得神经网络能够你和复杂的非线性模式</strong></p><ul class="lvl-0"><li class="lvl-2"><p>Sigmoid</p><ul class="lvl-2"><li class="lvl-4"><p>$$f(x) = \frac{1}{1+e^{-x}}$$</p></li><li class="lvl-4"><ul class="lvl-4"><li class="lvl-6">一般适用于二元分类问题，输出在0,1之间，在极值附近趋于0，反向传播梯度小时</li></ul></li></ul></li><li class="lvl-2"><p>Relu</p><ul class="lvl-2"><li class="lvl-4">$$f(x) = max(0,f(x))$$ 计算简单</li><li class="lvl-4">对于负值输入，梯度为0，梯度为0</li><li class="lvl-4">在0点处不可导，一般指定0处的导数为0或者使用ln(1+e^x)来取代，0处的导数值为0.5</li></ul></li><li class="lvl-2"><p>Gelu</p><ul class="lvl-2"><li class="lvl-4">高斯误差函数变种: $$GELU(x) = 0.5x(1+tanh(\sqrt{\frac{2}{\pi}}(x+0.044715x^3))$$</li><li class="lvl-4">在接近0的范围内呈线性，在其他地方具有非线形</li><li class="lvl-4">计算复杂度高，会增加训练时间</li></ul></li></ul><h3 id="损失函数">损失函数</h3><p>度量神经网络预测结果和真实标签的误差，常见损失函数有</p><ul class="lvl-0"><li class="lvl-2"><p>MSE（Mean Squared Error） L2Loss 均方误差</p><ul class="lvl-2"><li class="lvl-6">预测值与真实值见的误差服从标准高斯分布</li><li class="lvl-4">更快收敛</li></ul></li><li class="lvl-2"><p>MAE (Mean Absolute Error)</p><ul class="lvl-2"><li class="lvl-6">预测值与真实值之间服从拉普拉斯分布</li><li class="lvl-4">受异常值影响更小</li></ul></li><li class="lvl-2"><p>Cross-Entropy Loss 交叉熵损失函数</p><ul class="lvl-2"><li class="lvl-6">常用于多类别分类问题的模型的训练</li><li class="lvl-4"><img src="https://cdn.nlark.com/yuque/0/2023/png/33921914/1700403310342-a69a131f-0e78-4a49-a1f9-5c07ee13ae47.png" alt="img"></li><li class="lvl-4">避免梯度消失</li><li class="lvl-4">多分类问题效果好</li></ul></li><li class="lvl-2"><p>Huber Loss</p></li><li class="lvl-2"><ul class="lvl-2"><li class="lvl-4">MSE+MAE</li><li class="lvl-4">增加MSE鲁棒性，弥补MAE loss下降速度慢的问题</li><li class="lvl-4">额外设置超参数</li></ul></li><li class="lvl-2"><p><strong>NCE（Noise-Contrastive Estimation）：</strong></p></li><li class="lvl-2"><ul class="lvl-2"><li class="lvl-4"><strong>定义：</strong> NCE 是一种用于训练概率模型的损失函数，通过将真实样本与噪声样本进行对比，从而将问题转化为一个二分类问题。它的核心思想是通过引入负样本，简化了估计概率分布的问题。</li><li class="lvl-4"><strong>应用：</strong> 在对比学习中，NCE 可以被用作损失函数，通过最小化真实样本与负样本之间的差异来训练模型。NCE Loss 对于大规模数据集的训练效果较好。</li></ul></li><li class="lvl-2"><p><strong>InfoNCE（Information Noise-Contrastive Estimation）：</strong></p></li><li class="lvl-2"><ul class="lvl-2"><li class="lvl-4"><strong>定义：</strong> InfoNCE 是对 NCE 的扩展，通过使用信息论的概念，将 NCE 表述为最大化样本与负样本之间的互信息。这样的设计使得 InfoNCE 在处理大规模数据集时更加鲁棒。</li><li class="lvl-4"><strong>应用：</strong> 在对比学习中，InfoNCE Loss 用于最大化正例与负例之间的互信息，从而优化模型参数。</li><li class="lvl-4">NCE 可以被看作 InfoNCE 的一种特例，当 InfoNCE 的信息量项中的正则化项趋近于零时，它等价于 NCE。</li><li class="lvl-4">交叉熵可以被视为 NCE 在样本量趋于无穷时的极限情况。</li></ul></li></ul><h3 id="过拟合-欠拟合">过拟合 &amp; 欠拟合</h3><ul class="lvl-0"><li class="lvl-2"><p><strong>过拟合</strong>：在训练数据上好，在新数据上表现差</p><ul class="lvl-2"><li class="lvl-6"><p><strong>数据增强：增强数据的多样性</strong></p></li><li class="lvl-4"><p>交叉验证</p></li><li class="lvl-4"><p>削减模型层数</p></li><li class="lvl-4"><p><strong>正则化 （限制模型的参数大小）</strong></p><ul class="lvl-4"><li class="lvl-8">约束参数大小，降低过拟合，降低模型复杂度<ul class="lvl-6"><li class="lvl-10">L1 <strong>在模型的损失函数中添加模型权重的绝对值之和</strong><ul class="lvl-8"><li class="lvl-12"><strong>有助于一些权重为0，从而实现特征选择，减少模型的复杂性</strong></li><li class="lvl-12">适用于特征稀疏性强的情况</li></ul></li><li class="lvl-10">L2 <strong>在模型的损失函数中添加模型权重的平方和</strong><ul class="lvl-8"><li class="lvl-12"><strong>有助于防止权重过大，使模型更加稳定，使得所有特征对于预测都有一定贡献</strong></li><li class="lvl-12">适用于所有特征对预测有一定贡献的情况</li></ul></li></ul></li></ul></li><li class="lvl-4"><p>DropOut 剪枝</p><ul class="lvl-4"><li class="lvl-6">正则化和Dropout都应该是在网络的前半部分用（特征提取）</li></ul></li></ul></li><li class="lvl-2"><p><strong>欠拟合：在训练数据和新数据上表现都差</strong></p></li></ul><h3 id="梯度爆炸-消失">梯度爆炸&amp;消失</h3><ul class="lvl-0"><li class="lvl-2"><p>激活函数</p><ul class="lvl-2"><li class="lvl-6">Relu Gelu</li></ul></li><li class="lvl-2"><p>残差链接</p><ul class="lvl-2"><li class="lvl-6">缓解梯度消失的问题</li><li class="lvl-4">便于训练更深的网络</li><li class="lvl-4">提高模型的收敛速度</li></ul></li><li class="lvl-2"><p>Normalization</p><ul class="lvl-2"><li class="lvl-6">将模型特征的值统一到相似的范围内，加速模型的训练过程</li></ul></li><li class="lvl-2"><p>减小学习率</p></li></ul><h3 id="优化算法">优化算法</h3><p>通过迭代过程调整模型的参数，目的是最小化损失函数。</p><ul class="lvl-0"><li class="lvl-2"><p>梯度下降（GD gradient descent）</p><ul class="lvl-2"><li class="lvl-4"><p>BGD 每次迭代用训练集的所有数据</p><ul class="lvl-4"><li class="lvl-6">每次便利数据集制作一次梯度下降</li><li class="lvl-6">训练慢</li></ul></li><li class="lvl-4"><p>SGD每次迭代只考虑一个样本</p><ul class="lvl-4"><li class="lvl-6">下降迭代快，但可能非最优解</li></ul></li><li class="lvl-4"><p>mini-batch 迭代使用一批样本震荡</p></li><li class="lvl-4"><p>SGDM SGD with Momentum</p><ul class="lvl-4"><li class="lvl-6"><p>梯度下降过程可以引入惯性，如果前后梯度相反，减少震荡，如果前后方向相同，步伐加大，加快学习速度。<strong>动量项直接使用上一次迭代的梯度</strong></p></li><li class="lvl-6"><p>减小震荡：减小梯度更新的方差，使得模型参数更加平滑的更新</p></li><li class="lvl-6"><p>加速模型参数的收敛</p><ul class="lvl-6"><li class="lvl-8">$$v_{t+1} = \beta \cdot v_t+\eta \cdot \nabla J(\theta_t)$$</li><li class="lvl-8">$$\theta_{t+1} = \theta_t-v_{t+1}$$</li></ul></li><li class="lvl-6"><p><strong>问题</strong></p><ul class="lvl-6"><li class="lvl-8">容易导致局部最优解</li><li class="lvl-8">时刻t的梯度下降方向是由累积动量决定的</li><li class="lvl-8">梯度多次迭代都是一个方向，梯度增大导致梯度爆炸</li></ul></li></ul></li><li class="lvl-4"><p>NAG（Nesterov Acceleration）<strong>根据当前动量预估未来位置的梯度</strong></p><ul class="lvl-4"><li class="lvl-6"><p>$$v_{t+1} = \beta \cdot v_t+\eta \cdot \nabla J(\theta - \beta \cdot v_t)$$</p></li><li class="lvl-6"><p>$$\theta_{t+1} = \theta_t-v_{t+1}$$</p></li><li class="lvl-6"><p><strong>看动量累积后的梯度方向，先对参数进行一次估计，然后更新动量，再更新参数</strong></p></li><li class="lvl-6"><p>学习率是恒定的</p></li></ul></li><li class="lvl-4"><p>AdaGrad</p><ul class="lvl-4"><li class="lvl-6"><p><strong>累积全部梯度，对不同的参数使用不同的学习率</strong></p></li><li class="lvl-6"><p>频繁更新的参数学习率小，偶尔出现的参数学习率大</p></li><li class="lvl-6"><p>稀疏数据情况下表现好</p></li><li class="lvl-6"><p>实际情况下频繁更新的参数的学习率会很快衰减到0</p></li></ul></li><li class="lvl-4"><p>RMSProp （root mean square Prop）</p><ul class="lvl-4"><li class="lvl-6">为了解决AdaGrad学习率过度衰减的问题，只关注某一窗口的梯度</li></ul></li><li class="lvl-4"><p><strong>Adam (SGDM+RMSProp+修正)</strong></p><ul class="lvl-4"><li class="lvl-6"><strong>带动量的随机梯度下降</strong></li><li class="lvl-6"><strong>对于不同参数使用不同的学习率</strong></li><li class="lvl-6"><strong>为了防止累计梯度的问题，只关注某一窗口内的梯度</strong></li></ul></li></ul></li></ul><h3 id="Attention">Attention</h3><p><strong>在处理较长的序列数据时，传统的模型（如RNN）会面临梯度消失或梯度爆炸的问题，导致模型难以有效地捕捉长期依赖关系</strong>。<strong>Attention机制可以解决这个问题，使得模型能够更加灵活地关注不同时间步或不同位置的信息，从而提高了模型在处理序列数据时的能力。</strong></p><p>​                                 $$Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}}V) \quad d_k = K的列数$$</p><ul class="lvl-0"><li class="lvl-2"><p>Scale 为了防止矩阵内积过大</p></li><li class="lvl-2"><p>捕捉长序列中的重要关系，避免了传统模型难以处理长期依赖的问题</p></li><li class="lvl-2"><p>忽略序列中的冗余信息，从而提高模型的计算效率和泛化能力</p></li><li class="lvl-2"><p>Q：当前位置的向量，寻找其他位置 查询用于表示目前需要计算注意力的位置或信息。</p></li><li class="lvl-2"><p>K：所有位置的向量，衡量相关性 查询与不同位置的关联程度。键是用来与查询进行相似性计算的。</p></li><li class="lvl-2"><p>V：所有位置的向量，得到加权和 值是对应于每个键的实际信息</p></li></ul><h3 id="Self-attention">Self-attention</h3><h3 id="Transformer">Transformer</h3><ul class="lvl-0"><li class="lvl-2"><p>Transformer结构</p><ul class="lvl-2"><li class="lvl-4"><p>输入编码**：Word Embedding + Pos Embedding**</p><ul class="lvl-4"><li class="lvl-6"><p>WE：word2vec、Glove</p></li><li class="lvl-6"><p>PE: 适应比训练机里面所有句子更长的句子 且 更容易计算出相对位置</p></li></ul></li><li class="lvl-4"><p>Encoder block  不改变输入维度</p></li><li class="lvl-4"><p>encoder输出的编码矩阵传递到Decoder中，Decoder会根据当前翻译过的单词1～i翻译下一个单词，需要用mask遮住i+1之后的单词</p></li><li class="lvl-4"><p>Add表示残差链接层，用于防止网络退化</p></li><li class="lvl-4"><p>Norm：加快收敛</p></li><li class="lvl-4"><p>FFN：两层的全链接层：$$max(0,wx+b)w+b$$</p></li></ul></li><li class="lvl-2"><p>参数量计算</p><ul class="lvl-2"><li class="lvl-6"><p>self-attention: Wq,Wk,Wv,Wo, bias: 权重大小为 <strong>[h,h]</strong> ，bias为[h] 参数量为：<strong>4h*h + 4h</strong></p></li><li class="lvl-6"><p>MLP:两个线性层</p><ul class="lvl-4"><li class="lvl-10">维度从h–&gt;4h, bias 为4h</li><li class="lvl-10">维度从4h–&gt;h bias 为h</li><li class="lvl-10">参数量为<strong>8h*h+5h</strong></li></ul></li></ul></li><li class="lvl-2"><p>self-attention和MLP块各有一个LN</p></li><li class="lvl-2"><p>Layer Normalization <strong>综合考虑一层所有维度的输入，计算该层的平均输入值和输入方差，然后用同一个规范化操作来转换各个维度的输入。</strong></p><ul class="lvl-2"><li class="lvl-6">包含两个参数：缩放参数和平移参数，维度均为[h], 参数量共为4h</li></ul></li><li class="lvl-2"><p>每个transformer块共计参数：12h*h+13h</p><ul class="lvl-2"><li class="lvl-4">词嵌入矩阵参数：Vh</li><li class="lvl-4">因此Transformer模型课训练参数量：L(12h*h+13h)+Vh</li></ul></li></ul><h3 id="KV-cache">KV-cache</h3><h2 id="大模型结构">大模型结构</h2><h3 id="Position-Embedding">Position Embedding</h3><ul class="lvl-0"><li class="lvl-2"><p>Sinusoidal</p><ul class="lvl-2"><li class="lvl-4"><p>$$PE_{(pos,2i)}= sin(pos/10000^{2i/d_{model}})\quad PE_{(pos,2i+1)}= cos(pos/10000^{2i/d_{model}})$$</p></li><li class="lvl-4"><p>正弦和余弦函数的周期性特性确保了不同位置之间的编码关系是连续且平滑的。</p><ul class="lvl-4"><li class="lvl-6"><p>对于相邻位置，位置编码的差异较小，与两者之间的距离成正比。</p></li><li class="lvl-6"><p>对于相隔较远的位置，位置编码的差异较大，与两者之间的距离成正比。</p></li></ul></li></ul></li><li class="lvl-2"><p>RoPE</p><ul class="lvl-2"><li class="lvl-4">RoPE 相对位置编码，传统位置编码可能丢失相对位置信息</li><li class="lvl-4">我们通常会通过向量内积来计算注意力系数，如果能够对q k向量注入了位置信息，然后用更新的q，k向量做内积就会引入位置信息了。</li></ul></li></ul><h3 id="Bert">Bert</h3><p><strong>Bidirectional Encoder Representation from Transformer</strong></p><p>BERT的核心是使用Transformer模型进行预训练，通过大规模的无监督学习从大量的文本数据中学习通用的语言表示，然后在各种下游任务上进行微调，以适应特定的NLP任务</p><ul class="lvl-0"><li class="lvl-2"><p>预训练方式</p><ul class="lvl-2"><li class="lvl-4"><p>MLM：Masked Language Model</p><ul class="lvl-4"><li class="lvl-6">随机遮盖住一些文字<ul class="lvl-6"><li class="lvl-8">只会对15%masked的token进行预测，因此收敛速度会很慢</li><li class="lvl-8">特殊符号替换（特殊符号不在字典里）</li><li class="lvl-8">随机文字替换</li></ul></li></ul></li><li class="lvl-4"><p>NSP: Next Sentence Predict</p></li></ul></li><li class="lvl-2"><p>Encoder架构：</p></li></ul><h3 id="VIT">VIT</h3><p>Vision Transformer(ViT)将输入图片拆分成16x16个patches，每个patch做一次线性变换降维同时嵌入位置信息，然后送入Transformer，避免了像素级attention的运算。类似BERT[class]标记位的设置，ViT在Transformer输入序列前增加了一个额外可学习的[class]标记位，并且该位置的Transformer Encoder输出作为图像特征。</p><p>添加这个 class token 的目的是因为，ViT 模型将这个 class token 在 Transformer Encoder 的输出当做是模型对输入图片的编码特征，用于后续输入 MLP 模块中与图片 label 进行 loss 计算。</p><p>步骤如下：</p><ol><li class="lvl-3"><p>将一张图片分成patches；</p></li><li class="lvl-3"><p>将patches铺平；</p></li><li class="lvl-3"><p>将铺平后的patches的线性映射到更低维的空间；</p></li><li class="lvl-3"><p>添加位置embedding编码信息；</p></li><li class="lvl-3"><p>将图像序列数据送入标准Transformer encoder中去；</p></li><li class="lvl-3"><p>在较大的数据集上预训练；</p></li><li class="lvl-3"><p>在下游数据集上微调用于图像分类。</p></li></ol><h3 id="Finetune">Finetune</h3><p>微调(Finetune)就是在一个预训练的大模型上，使用一些特定领域的数据再次进行训练，从而让预训练模型能够适应特定领域、特定任务。Fine-tune则是在预训练模型的基础上，只微调最后几层或最后一层，以适应新任务。这种方法适用于大规模数据集和比较复杂的任务。</p><ul class="lvl-0"><li class="lvl-2"><p>SFT</p></li><li class="lvl-2"><p>PEFT</p></li></ul><h4 id="LoRA">LoRA</h4><p>**LoRA 的基本原理是冻结预训练好的模型权重参数，在冻结原模型参数的情况下，通过往模型中加入额外的网络层，并只训练这些新增的网络层参数。**新增参数数量较少，这样不仅 finetune 的成本显著下降，还能获得和全模型参数参与微调类似的效果。</p><p>​ $$h = Wx+BAx$$</p><ul class="lvl-0"><li class="lvl-2"><p>基本原理:冻结原模型。原有层模型拆分为两个低秩矩阵，A（高斯初始化）和B（0）（保证在初始阶段，BA的结果为0，不起作用）</p></li><li class="lvl-2"><p>数学：低秩矩阵，模型参数量少了，但是效果不变，数学上采用的是SVD</p></li><li class="lvl-2"><p>LoRA的本质是把秩当成超参，让模型去学习低秩矩阵。</p></li><li class="lvl-2"><p>部署的时候可以把<strong>参数矩阵与冻结的参数矩阵合并,推理时间并不会增加</strong></p></li><li class="lvl-2"><p>对单一的层使用较大的秩，对更多的层使用较小的秩的效果更好。</p><ul class="lvl-2"><li class="lvl-6">适配更多的权重矩阵比适配具有较大秩的单一类型的权重矩阵表现更好。</li><li class="lvl-4">增加秩不一定能够覆盖一个更有意义的子空间，一个低秩的适配矩阵可能已经足够了</li></ul></li></ul><h4 id="Prefix-tuning">Prefix-tuning</h4><p>问题在于最终的性能对人工设计的template的特别敏感：加一个词或者少一个词，或者变动位置，都会造成很大的变化，<strong>所以这种离散化的token的搜索出来的结果可能并不是最优的。</strong></p><p>其使用连续的virtual token embedding来代替离散的token。</p><p>在输入token之前构造一段任务相关的virtual tokens作为Prefix相当于对于transformer的每一层 (不只是输入层，且每一层transformer的输入不是从上一层输出，而是随机初始化的embedding作为输入，都在真实的句子表征前面插入若干个连续的可训练的&quot;virtual token&quot; embedding，这些伪token不必是词表中真实的词，而只是若干个可调的自由参数</p><p>​为了防止直接更新Prefix的参数导致训练不稳定的情况，特在Prefix层前面加了MLP结构(<em>相当于将Prefix分解为更小维度的Input与MLP的组合后输出的结果</em>)</p><h3 id="并行方式">并行方式</h3><h4 id="Tensor-Para">Tensor Para</h4><p>对矩阵乘法进行拆分，从而降低模型对显存的需求 [Alpa]</p><ul class="lvl-0"><li class="lvl-2"><p>对MLP层</p><ul class="lvl-2"><li class="lvl-5"><p>MLP一般是两个FFN层</p></li><li class="lvl-6"><p>按行切分权重，就需要对输入按列进行切分</p></li><li class="lvl-4"><p>按列切分权重，不需要对输入进行切分</p></li><li class="lvl-4"><p>Megatron：</p><pre><code class="hljs">- 第一层：X输入拷贝在两块GPU上，独对权重矩阵A按照列切分，独立进行forward</code></pre><ul class="lvl-4"><li class="lvl-6"><p>第二层：对权重矩阵B按照行切分，此时两块GPU上的各自有最终结果的一部分，做一次ALLReduce，相加产生结果</p></li><li class="lvl-6"><p>目的保证各GPU上计算相互独立，减少通信量（A进行行切分会导致多一次ALLReduce，这是因为GeLU函数非线性的，GELU(x1+x2)!=GELU(x1)+GELU(x2)）</p></li></ul></li></ul></li><li class="lvl-2"><p>对attention层</p><ul class="lvl-2"><li class="lvl-6">对KQV矩阵按列进行切割，每个头放在一块GPU上做并行计算</li><li class="lvl-4">对线性层B按照行切割</li></ul></li><li class="lvl-2"><p>输入Embedding</p><ul class="lvl-2"><li class="lvl-6">word embedding 【v，h】词表大小</li><li class="lvl-4">词表在两块GPU上，输入访问后再做all-reduce</li></ul></li><li class="lvl-2"><p>输出Embedding</p><ul class="lvl-2"><li class="lvl-6">一般来说，输入层和输出层共用一个word Embbeding</li><li class="lvl-4">训练的时候，在输入层和输出层会对word embedding做两次梯度更新，必须保证用两次梯度的总和进行更新，当输入和输出层在不同GPU上时，权重更新在前（All reduce）</li></ul></li></ul><h4 id="pipeline-Para">pipeline Para</h4><ul class="lvl-0"><li class="lvl-2"><p>将模型的不同层放到不同的GPU上，按照流水线的方式进行推理。</p></li><li class="lvl-2"><p>【Gpipe】</p><ul class="lvl-2"><li class="lvl-6">batch–&gt;mini_batch</li><li class="lvl-4">整个batch计算完成后才会反向传播</li><li class="lvl-4">active ckpt 解决内存不足，向后更新的时候重新计算一遍forward</li></ul></li><li class="lvl-2"><p>【pipedream】</p><ul class="lvl-2"><li class="lvl-6">每个microbatch前向传播完成后，立即进入反向传播阶段</li></ul></li></ul><h4 id="Data-Para">Data Para</h4><ul class="lvl-0"><li class="lvl-2"><p>每块GPU都存储一份完整的模型，造成冗余，可以通过ZeRO技术进行优化</p></li><li class="lvl-2"><p>通讯开销大：server需要和每一个worker进行梯度传输，用于更新，带宽易成为瓶颈</p></li></ul>]]></content>
    
    
    <categories>
      
      <category>面经</category>
      
    </categories>
    
    
    <tags>
      
      <tag>大模型 实习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>实习面经</title>
    <link href="/2023/12/18/test1/"/>
    <url>/2023/12/18/test1/</url>
    
    <content type="html"><![CDATA[<h1><strong>实习面试记录</strong></h1><p>​本次面试期间为12.05～12.14，投递主要方式为Boss直聘和牛客网站邮箱投递，面试了字节、快手、百度、好未来、小米等中大厂及个别小厂。其中快手、好未来offer；字节一面挂、百度二面后未知，小米一面后拒绝二面、小厂均过。</p><h2 id="百度">百度</h2><ul class="lvl-0"><li class="lvl-2"><p>百度一面</p><ul class="lvl-2"><li class="lvl-4">时间：2023.12.07</li><li class="lvl-4">岗位：内容解析与理解及资源与用商垂类搜索组_机器学习实习研发工程师</li><li class="lvl-4">面试内容：<ul class="lvl-4"><li class="lvl-6">项目</li><li class="lvl-6">算法题：<strong>课程表2 leetcode210</strong></li></ul></li><li class="lvl-4">结果：一面通过</li></ul></li><li class="lvl-2"><p>百度二面</p><ul class="lvl-2"><li class="lvl-4"><p>时间：2023.12.11</p></li><li class="lvl-4"><p>岗位：内容解析与理解及资源与用商垂类搜索组_机器学习实习研发工程师</p></li><li class="lvl-4"><p>面试内容</p><ul class="lvl-4"><li class="lvl-6">项目很详细</li><li class="lvl-6">反问：你们做的是什么：主要是文本解析</li><li class="lvl-6">算法题：<strong>53 最大连续子数组和</strong></li></ul></li><li class="lvl-4"><p>结果：暂时未知，大概率G，方向不匹配</p></li></ul></li></ul><h2 id="快手">快手</h2><ul class="lvl-0"><li class="lvl-2"><p>快手一面</p><ul class="lvl-2"><li class="lvl-4"><p>时间：2023.12.12</p></li><li class="lvl-4"><p>岗位：算法实习生（数据方向）－【效率工程部】</p></li><li class="lvl-4"><p>面试内容：</p><ul class="lvl-4"><li class="lvl-6">项目</li><li class="lvl-6">数据中台，数据量大，做数据大模型</li><li class="lvl-6">无算法题</li></ul></li></ul></li><li class="lvl-2"><p>结果：过</p></li><li class="lvl-2"><p>快手二面</p><ul class="lvl-2"><li class="lvl-4">时间：2023.12.12</li><li class="lvl-4">岗位：算法实习生（数据方向）－【效率工程部】</li><li class="lvl-4">面试内容：<ul class="lvl-4"><li class="lvl-6">数据分析的需求、数据生成内容、数据资产门户</li><li class="lvl-6">调研前沿大模型，做内容总结</li></ul></li><li class="lvl-4">结果：offer</li></ul></li></ul><h2 id="好未来">好未来</h2><ul class="lvl-0"><li class="lvl-2"><p>好未来一面</p><ul class="lvl-2"><li class="lvl-4"><p>时间： 2023.12.12</p></li><li class="lvl-4"><p>岗位：大模型推理优化架构</p></li><li class="lvl-4"><p>面试内容：</p><ul class="lvl-4"><li class="lvl-6">项目</li><li class="lvl-6">问的都是前沿知识 Flash Attention Page Attention vLLM</li><li class="lvl-6">算法题：<strong>最长公共子序列求子序列是什么</strong></li></ul></li><li class="lvl-4"><p>结果：过</p></li></ul></li><li class="lvl-2"><p>好未来二面</p><ul class="lvl-2"><li class="lvl-4">时间：2023.12.13<ul class="lvl-4"><li class="lvl-6">岗位：大模型推理优化架构</li><li class="lvl-6">面试内容：</li><li class="lvl-6">项目</li><li class="lvl-6">问的都是前沿知识 Flash Attention Page Attention vLLM</li><li class="lvl-6">算法题：<strong>无重复数字的全排列</strong></li></ul></li><li class="lvl-4">结果：offer</li></ul></li></ul><h2 id="小米">小米</h2><ul class="lvl-0"><li class="lvl-2"><p>小米一面</p><ul class="lvl-2"><li class="lvl-4">时间：2023.12.14</li><li class="lvl-4">岗位：推荐架构</li><li class="lvl-4">面试内容：<ul class="lvl-4"><li class="lvl-6">项目</li><li class="lvl-6">反问：做什么，主要是搜索，APP上搜索框</li><li class="lvl-6">算法题：<strong>leetcode 1267统计参与通信的服务器</strong></li></ul></li><li class="lvl-4">结果：过</li></ul></li></ul>]]></content>
    
    
    <categories>
      
      <category>面试</category>
      
    </categories>
    
    
    <tags>
      
      <tag>实习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hexo template</title>
    <link href="/2023/05/08/template/"/>
    <url>/2023/05/08/template/</url>
    
    <content type="html"><![CDATA[<h1>Fluid Template</h1><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs markdown">Front-matter:<br><span class="hljs-code">title:</span><br><span class="hljs-code">tags:</span><br><span class="hljs-code">excerpt: 摘要</span><br><span class="hljs-code">sticky: 重要程度</span><br><span class="hljs-code">index_img: 封面图必须是source文件夹下的图片或外部链接</span><br><span class="hljs-code">banner_img: 顶部图</span><br></code></pre></td></tr></table></figure><h2 id="便签">便签</h2><ul class="lvl-0"><li class="lvl-2"><p>标签</p></li></ul><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs markdown">Note种类：success,primary,danger,secondary,warning,info,light<br>&#123;% note success %&#125;<br>文字 或者 <span class="hljs-code">`markdown`</span> 均可 <br>&#123;% endnote %&#125;<br></code></pre></td></tr></table></figure><div class="note note-success">            <p>​Example : success</p>          </div><ul class="lvl-0"><li class="lvl-2"><p>行内标签</p></li></ul><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs markdown">&#123;% label primary @text %&#125;<br></code></pre></td></tr></table></figure><span class="label label-info">Example Info</span><h2 id="按钮">按钮</h2><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs text">&#123;% btn url, text, title %&#125;<br></code></pre></td></tr></table></figure><a class="btn" href="https://hexo.fluid-dev.com/docs/guide/#tag-%E6%8F%92%E4%BB%B6"  title="Hexo" target="_blank">Hexo配置指南</a>]]></content>
    
    
    <categories>
      
      <category>Tutorial</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Hexo</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>使用Hexo+github+Fluid+LeanCloud搭建个人Blog</title>
    <link href="/2023/05/08/%E4%BD%BF%E7%94%A8Hexo+github+Fluid+LeanCloud%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BABlog/"/>
    <url>/2023/05/08/%E4%BD%BF%E7%94%A8Hexo+github+Fluid+LeanCloud%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BABlog/</url>
    
    <content type="html"><![CDATA[<h1>配置个人blog</h1><h2 id="环境配置">环境配置</h2><ul class="lvl-0"><li class="lvl-2"><p>安装本地环境</p></li></ul><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs markdown"><span class="hljs-section"># 安装node.js</span><br>brew install node<br><span class="hljs-section"># 安装git</span><br>brew install git<br><span class="hljs-section"># 安装hexo</span><br>npm install -g hexo-cli<br></code></pre></td></tr></table></figure><p>​</p><ul class="lvl-0"><li class="lvl-2"><p>安装Fluid主题</p></li></ul><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs markdown"><span class="hljs-section"># init project</span><br>hexo init 【项目名】<br>cd 【项目名】<br>npm install<br><span class="hljs-section"># git clone</span><br>git clone https://github.com/iissnan/hexo-theme-next themes/fluid<br></code></pre></td></tr></table></figure><ul class="lvl-0"><li class="lvl-2"><p>配置流量统计</p></li></ul><p>​从<a class="btn" href="https://console.leancloud.cn/"  title="Leancloud" target="_blank">Leancloud</a> 注册账号，新建项目，并在设置—应用凭证中获取<strong>AppID、AppKey、REST API</strong>。</p><ul class="lvl-0"><li class="lvl-2"><p>插件</p></li></ul><p><code>npm install hexo-deployer-git --save</code></p><p><code>npm i hexo-renderer-markdown-it --save</code></p><h2 id="github代理">github代理</h2><ol><li class="lvl-3"><p>新建github项目，命名为 <strong><mark>&lt;用户名&gt;.github.io</mark></strong></p></li><li class="lvl-3"><p>hexo clean</p></li><li class="lvl-3"><p>hexo g</p></li><li class="lvl-3"><p>hexo d</p></li></ol><h2 id="创建页">创建页</h2><p>​使用<code>hexo new [模版] [文件名]</code> 来创建新的markdown。</p><p>​模版存在<code>scafflolds</code>文件夹下。</p><p>​修改<code>post_asset_folder: true</code>使得在创建.md文件同时创建一个文件夹保存图片，在md文件引用图片时，只需要文件名，这可能导致在md文件中报错，但在网站上并不会报错。</p><h2 id="Fluid配置">Fluid配置</h2><p><a href="https://hexo.fluid-dev.com/docs/guide/">Hexo - Fluid 用户手册</a></p>]]></content>
    
    
    <categories>
      
      <category>Tutorial</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Hexo</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
